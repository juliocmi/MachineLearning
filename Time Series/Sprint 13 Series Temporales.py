#!/usr/bin/env python
# coding: utf-8

# # Proyecto Sweet Lift Taxi
# ---
# 
# - **Fecha:** 30-Abr-23
# - **Notebook by:** Data Scientist Julio C칠sar Mart칤nez Izaguirre
# - **Senior Data Scientist:** by Alfonso Tobar

# # Tabla de Contenido
# ---
# 
# 1. Instrucciones del Proyecto
# 2. Descripci칩n de los Datos
# 3. Preparaci칩n
# 4. An치lsis de Procesos Estoc치stico
# 5. Formaci칩n de Modelos
# 6. Prueba
# 7. Conclusiones
# 8. Agradecimientos

# # Licencia
# ---
# 
# Copyright @2023 by Julio C칠sar Mart칤nez Izaguirre
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License

# # Descripci칩n del proyecto
# ---
# 
# La compa침칤a **Sweet Lift Taxi** ha recopilado datos hist칩ricos sobre pedidos de taxis en los aeropuertos. Para atraer a m치s conductores durante las horas pico, necesitamos predecir la cantidad de pedidos de taxis para la pr칩xima hora. Construye un modelo para dicha predicci칩n.
# 
# La m칠trica **RECM** en el conjunto de prueba no debe ser superior a 48.
# 
# ## || Instrucciones del proyecto.
# ---
# 
# 1. Descarga los datos y haz el remuestreo por una hora.
# 2. Analiza los datos
# 3. Entrena diferentes modelos con diferentes hiperpar치metros. La muestra de prueba debe ser el 10% del conjunto de datos inicial.
# 4. Prueba los datos usando la muestra de prueba y proporciona una conclusi칩n.
# 
# ## || Descripci칩n de los datos
# ---
# 
# Los datos se almacenan en el archivo `taxi.csv`. 	
# El n칰mero de pedidos est치 en la columna `num_orders`.

# ## || Preparaci칩n
# ---

# Importamos librerias

# In[1]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
import statsmodels.api as sm
import lightgbm
import sklearn
import time

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.statespace.sarimax import SARIMAX
from lightgbm import LGBMRegressor
from xgboost import XGBRegressor
from matplotlib import pyplot
from pylab import rcParams

# graficos incrustados
plt.style.use('fivethirtyeight')
get_ipython().run_line_magic('matplotlib', 'inline')

# parametros esteticos de seaborn
sns.set_palette("deep", desat=.6)
sns.set_context(rc={"figure.figsize": (12, 4)})

start_time = time.time()

print("Pandas Version:      ", pd.__version__)
print("Numpy Version :      ", np.__version__)
print("Seaborn Versi칩n:     ", sns.__version__)
print("Scikit-Learn Version:", sklearn.__version__)
print("LightGBM Version:    ", lightgbm.__version__)


# Ahora vamos a importar nuestro **Dataframe** donde utilizaremos la transformaci칩n de datos de tipo **object** a tipo **datetime** durante la lectura del set y al mismo tiempo asignaremos las fechas al 칤ndice de la tabla.

# In[2]:


sweetlift = pd.read_csv('/datasets/taxi.csv', parse_dates=[0])


# In[3]:


# Vemos Dataframe
print("Tipo de columnas:       ", sweetlift.dtypes)
print("Tama침o del set de datos:", sweetlift.shape)
sweetlift.head()


# Informaci칩n general.

# In[4]:


sweetlift.info()


# **OBSERVACIONES**
# 
# El dataframe de **Sweetlift** se compone de una columna y 26,496 filas. No se han detectado valores **nulos** o ausentes y al mismo tiempo se han ordenado los datos de acuerdo al tipo de **fecha y hora**. Adem치s al importar los datos hemos convertido las columnas del tipo `objet` al tipo `datetime`.

# ## || An치lisis de Proceso Estoc치stico
# 
# Vamos a realizar un poco de an치lisis estad칤stico con el objetivo de encontrar alguna inconsistencia en los datos y que al mismo tiempo ver qu칠 otra informaci칩n podemos obtener.

# In[5]:


sweetlift.describe()


# In[6]:


print("Fecha Inicio:", sweetlift.datetime.min())
print("Fecha Fin:   ", sweetlift.datetime.max())


# ### Remuestreo
# 
# Ahora que sabemos cu치l es el inicio y el final de nuestro set vamos a realizar un remuestreo, esto significa cambiar el intervalo con los valores de la serie. Vamos a realizar este procedimiento a trav칠s de la funci칩n **Resample** y agruparemos toda la informaci칩n por horas debido a que este es el 칤ndicador que a **Sweetlift** m치s le interesa.

# In[7]:


# Vamos a ordenar las fechas por hora
data = sweetlift.sort_index()
data = data.set_index('datetime').resample('H').sum()
data.head()


# In[8]:


# Gr치ficamos la nueva tabla
data.plot(figsize=(15, 3),color = 'blue')
plt.title('Taxi Sweetlift por Hora')
plt.ylabel('Frecuencia')
plt.show()


# ### Media M칩vil y Desviaci칩n Est치ndar
# 
# Ahora vamos a utilizar un m칠todo para suavizar los datos llamado **Media M칩vil**, este m칠todo consiste en encontrar los valores menos suceptibles a fluctuaciones, es decir, la media aritm칠tica. Para encontrar la media m칩vil utilizaremos la funci칩n `rolling` y enseguida especificar칠mos su tama침o.

# In[9]:


# Calcular media m칩vil y agregar nueva columna al dataset
data['mean'] = data['num_orders'].rolling(7).mean()
data['std']  = data['num_orders'].rolling(7).std()
print(data.head(10))

data.plot(figsize=(15, 3),color = ['darkblue','orange','green'])
plt.title('Taxi Sweetlift por Hora')
plt.ylabel('Frecuencia')
plt.show();


# ### Descomposici칩n STL
# 
# Para comprender mejor nuestra serie temporal vamos analizar las tendencias y la estacionalidad. Recordemos que una **Tendencia** es un cambio ligero del valor medio de la serie sin repetir patrones. Por otro lado la **Estacionalidad** identifica patrones que se repiten de forma ciclica en una serie temporal. La tendencia y la estacionalidad dependen de la escala de los datos, es decir, no podemos ver patrones que se repiten todos los a침os si solo hay datos de un a침o. Para estudiar la tendencia y estacionalidad de nuestra serie temporal vamos a utilizar el m칩dulo de `tsa.seasonal` de la librer칤a `statsmodels` que contienen la funci칩n **seasonal_decompose()**.

# Una serie temporal la podemos descomponer en tres partes:
# 
# - **Tendencia** ($T$): Trayectoria de los datos en el tiempo (direcci칩n positiva o negativa).
# - **Estacionalidad** ($S$): Fluctuaciones regulares y predecibles en un periodo determinado (anual, semestral, etc.)
# - **Ruido** ($e$): error intr칤nsico al tomar una serie temporal (instrumentos, medici칩n humana, etc.)
# 
# En muchas ocasiones no es posible descomponer el **proceso estoc치stico** puesto que la muestra obtenida no presenta un comportamiento ciclico o repetitivo en el periodo de tiempo analizado. Por otra parte esta descomposici칩n puede realizarse de dos formas diferentes:
# 
# - **Aditiva**
# 
# $X_{t}=T_{t}+S_{t}+e_{t}$
# 
# - **Multiplicativa**
# 
# $X_{t}=T_{t} * S_{t} * e_{t}$

# In[10]:


# Decomposici칩n STL Forma A
#--------------------------
rcParams['figure.figsize'] = 20, 10
decomposition2 = sm.tsa.seasonal_decompose(data['num_orders'], model='aditive')
fig = decomposition2.plot()
plt.show()


# In[11]:


# Decomposici칩n STL Forma B
#--------------------------
decomposed = seasonal_decompose(data['num_orders'])

plt.figure(figsize=(15, 10))
plt.subplot(311)
# Para mostrar el gr치fico de manera correcta, especificamos su
# eje ax igual a plt.gca() (gca = obtener el eje actual)
decomposed.trend.plot(ax=plt.gca(),color = 'darkblue')
plt.title('Trend')
plt.subplot(312)
decomposed.seasonal.plot(ax=plt.gca(), color = 'darkblue')
plt.title('Seasonality')
plt.subplot(313)
decomposed.resid.plot(ax=plt.gca(), color = 'darkblue')
plt.title('Residuals')
plt.show();


# Analicemos cada uno de estos gr치ficos:
# 
# - **gr치fico 01** (serie original): este gr치fico simplemente nos muestra la serie original graficada en el tiempo.
# - **gr치fico 02** (tendencia): este gr치fico nos muestra la tendencia de la serie, para este caso, se tiene una tendencial lineal positiva.
# - **gr치fico 03** (estacionariedad): este gr치fico nos muestra la estacionariedad de la serie, para este caso, se muestra una estacionariedad mensual, esta estacionariedad se puede ver como una forma estacionaria.
# - **gr치fico 04** (error): este gr치fico nos muestra el error de la serie, para este caso, el error oscila entre -100 y 100. En general se busca que el error sea siempre lo m치s peque침o posible y que tenga el comportamiento de una distribuci칩n normal. Cuando el error sigue una distribuci칩n normal con media cero y varianza 1, se dice que el error es un **ruido blanco**.
# 
# **RUIDO BLANCO**
# 
# Veamos como es un ruido blanco.

# In[12]:


# Grafico: lineplot 
#------------------
np.random.seed(42) # fijar semilla

mean = 0
std = 1 
num_samples = 300


samples = np.random.normal(mean, std, size=num_samples)

plt.plot(samples)
plt.title("Ruido blanco: N(0,1)")
plt.show()


# Podemos ver que el ruido blanco oscila sobre el valor 0 y tiene una varianza constante (igual a 1). 
# 
# Ahora veamos el histograma de un ruido blanco.

# In[13]:


# Grafico: Histograma
#--------------------
plt.hist(samples,bins = 10)
plt.title("Ruido blanco: N(0,1)")
plt.show()


# EL histograma de una variable normal, se caracteriza por esa forma de campana sim칠trica entorno a un valor, en este caso, entorno al valor 0.

# ### Serie Estacionaria
# 
# Como sabemos nuestro **Proceso Estoc치stico** se compone de una media y una varianza, y estos valores cambian. Se dice que nuestra serie es **Estacionaria** si su distribuci칩n no cambia con el tiempo, por el contrario si la distribuci칩n del proceso estoc치stico cambia entonces no es estacionaria. La manera m치s simple es gr치ficarla e inferir el comportamiento de esta. La ventaja que este m칠todo es r치pido, sin embargo, se encuentra sesgado por el criterio del ojo humano. Por otro lado existen algunas alternativas que aqu칤 presentamos:
# 
# **Autocorrelaci칩n (ACF) y autocorrelaci칩n parcial PACF**
# 
# Definamos a grandes rasgos estos conceptos:
# - **Funci칩n de autocorrelaci칩n (ACF)**. En el retardo  洧녲 , es la autocorrelaci칩n entre los valores de las series que se encuentran a  洧녲  intervalos de distancia.
# - **Funci칩n de autocorrelaci칩n parcial (PACF)**. En el retardo  洧녲 , es la autocorrelaci칩n entre los valores de las series que se encuentran a  洧녲 intervalos de distancia, teniendo en cuenta los valores de los intervalos intermedios.
# 
# Si la serie temporal es estacionaria, los gr치ficos ACF / PACF mostrar치n una r치pida disminuci칩n de la correlaci칩n despu칠s de un peque침o retraso entre los puntos.
# Gr치fiquemos la acf y pacf de nuestra serie temporal ocupando los comandos plot_acf y plot_pacf, respectivamente.

# In[14]:


pyplot.figure(figsize=(12,9))

# ACF
#----
pyplot.subplot(211)
plot_acf(data['num_orders'], ax=pyplot.gca(), lags = 30)

#PACF
#----
pyplot.subplot(212)
plot_pacf(data['num_orders'], ax=pyplot.gca(), lags = 30)
pyplot.show()


# Se observa de ambas imagenes, que estas decaen r치pidamente a cero, por lo cual se puede decir que la serie en estudio es estacionaria.
# 
# **PRUEBA DE DICKEY-FULLER**
# 
# En estad칤stica, la prueba Dickey-Fuller prueba la hip칩tesis nula de que una ra칤z unitaria est치 presente en un modelo autorregresivo. La hip칩tesis alternativa es diferente seg칰n la versi칩n de la prueba utilizada, pero generalmente es estacionariedad o tendencia-estacionaria. Lleva el nombre de los estad칤sticos David Dickey y Wayne Fuller, quienes desarrollaron la prueba en 1979.
# 
# Para efectos pr치ticos, para el caso de estacionariedad se puede definir el test como:
# Hip칩tesis nula: la serie temporal no es estacionaria.
# Hip칩tesis alternativa: la serie temporal es alternativa.
# Rechazar la hip칩tesis nula (es decir, un valor p muy bajo) indicar치 estacionariedad

# In[15]:


#Test Dickey-Fuller:
#-------------------
print ('Resultados del test de Dickey-Fuller:')
print ('------------------------------------------')
dftest = adfuller(data['num_orders'], autolag='AIC')
dfoutput = pd.Series(dftest[0:4], 
                     index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])

print(dfoutput)


# Dado que el **p-value** es 0.028940, se concluye que la serie temporal es estacionaria.

# ## || Formaci칩n
# ---
# 
# El objetivo del **Pron칩stico de Series Temporales** es desarrollar un modelo que prediga los valores futuros de una serie temporal con base en datos anteriores. El periodo en el futuro para el que se prepara el pron칩stico se conoce como **Horizonte de Pron칩stico**. Si los valores de una serie temporal o de la funci칩n $X(t)$ (donde t = tiempo) son n칰meros, entonces nos enfrentamos a una tarea de regresi칩n para la serie temporal, este es nuestro caso.
# 
# Comencemos escribiendo una funci칩n para crear caracter칤sticas.

# In[16]:


# Creamos Copia del Dataset
#--------------------------
df = data.copy()
df.drop(['mean', 'std'], axis=1, inplace=True)


# In[17]:


# Funci칩n para Crear Caracter칤sticas
#-----------------------------------
def make_features(data, max_lag, rolling_mean_size):
    # Agregar nuevas caracter칤sticas
    data['year'] = data.index.year
    data['month'] = data.index.month
    data['day'] = data.index.day
    data['dayofweek'] = data.index.dayofweek
    
    # Agregar y calcular nuevos valores de desfase
    for lag in range(1, max_lag + 1):
        data['lag_{}'.format(lag)] = data['num_orders'].shift(lag)

    ## Agregar Media M칩vil
    data['rolling_mean'] = data['num_orders'].shift().rolling(rolling_mean_size).mean()


# Aplicamos Funci칩n
#------------------
make_features(df, 3, 3)


# In[18]:


# Comprobamos las caracter칤sticas
df = df.dropna()
df.head()


# ### Train Test Split
# 
# Vamos a dividir los datos en conjuntos de **Entrenamiento y Prueba** y a obtener las caracter칤sticas de este 칰ltimo del conjunto de entrenamiento. Los valores restantes y la **Media M칩vil** se pueden calcular a partir de datos anteriores. Las caracter칤sticas de los primeros valores del conjunto de prueba se encuentran al final del conjunto de entrenamiento.
# 
# Es imposible obtener las caracter칤sticas para los primeros valores del conjunto de entrenamiento porque no hay datos previos con los cuales trabajar. En los ejercicios anteriores, los valores de estos atributos fueron NaN. Estos se deben eliminar. Finalmente dejaremos como prueba el **10% de los datos** a petici칩n de **Sweetlift**.

# In[19]:


# Realizamos divisi칩n de datos
#-----------------------------
train, test = train_test_split(df, shuffle=False, test_size=0.1)

X_train = train.drop(['num_orders'], axis=1)
y_train = train['num_orders']
X_test  = test.drop(['num_orders'], axis=1)
y_test  = test['num_orders']

# Tama침o de los sets
#-------------------
print("Set de Entrenamiento:", X_train.shape, y_train.shape)
print("Set de Prueba:", X_test.shape, y_test.shape)
print()
# Comprobamos el orden correcto
#------------------------------
print("Fecha Inicial:", X_train.index.min(), "Fecha Final:", X_train.index.max())
print("Fecha Inicial:", X_test.index.min(), "Fecha Final", X_test.index.max())


# In[20]:


# Visualizamos el orden correcto
#-------------------------------
fig, ax = plt.subplots(figsize=(15,5))
y_train.plot(ax=ax)
y_test.plot(ax=ax)
ax.axvline('2018-08-13 14:00:00', color='black', ls='--')
plt.title('Separaci칩n de Datos')
plt.legend(['training set', 'test set'])
plt.show();


# ### Regresi칩n L칤neal
# 
# **TIME SERIES SPLIT**

# In[21]:


# Definimos Time Series Split
#----------------------------
tscv = TimeSeriesSplit(n_splits=7)

preds  = []
scores = []

for i,(train_idx, test_idx) in enumerate(tscv.split(df)):
    train = df.iloc[train_idx]
    test  = df.iloc[test_idx] 
    
    features = ['year', 'month', 'day', 'dayofweek', 'lag_1', 'lag_2', 'lag_3', 'rolling_mean']
    target = 'num_orders'
    
    X_train = train[features]
    y_train = train[target]
    X_test = test[features]
    y_test = test[target]
        
    regresion_model = LinearRegression()
    regresion_model.fit(X_train, y_train)
    
    y_pred = regresion_model.predict(X_test)
    preds.append(y_pred)
    score = mean_squared_error(y_test, y_pred, squared=False)
    scores.append(score)
    print(f'score for fold {i}:', scores)

# M칠trica Final
#--------------
RECM_lr = np.mean(scores)
print()
print('RECM Final:', RECM_lr)


# # Random Forest Regressor
# 
# Ahora probaremos el modelo del bosque aleatorio. Recordemos que Los 치rboles de decisi칩n se utilizan tanto para problemas de regresi칩n como de clasificaci칩n. Un **Bosque Aleatorio** se compone de muchos 치rboles de decisi칩n. Fluyen visualmente como 치rboles, de ah칤 el nombre, y en el caso de regresi칩n, comienzan con la ra칤z del 치rbol y siguen divisiones basadas en resultados variables hasta que se alcanza un nodo hoja y se da el resultado.
# 
# En el siguiente paso vamos a entrenar el modelo configurando un ajuste de hiperpar치metros a trav칠s de la herramienta de **GridSearch** que viene inclu칤da en la librer칤a de **Scikit-Learn**. De esta manera podremos obtener los mejores resultados para nuestro modelo.

# In[22]:


# Establecer valores para hiperpar치metros
#-------------------------------------------
h_params = {'n_estimators' : [100, 200, 500],
            'max_depth' : [2, 4, 6], 
            'random_state' : [42]}

# Llamamos a GridSearchCV y colocamos los valores de los hiperpar치metros
# La m칠trica RECM y el valor de validaci칩n cruzada
#-----------------------------------------------------------------------
gridSearch_rf = GridSearchCV(estimator = RandomForestRegressor(), 
                             param_grid = h_params, 
                             scoring = 'neg_root_mean_squared_error', 
                             cv=tscv)


# In[23]:


# Entrenamiento de GridSearch
#----------------------------
start_tr = time.time()
gridSearch_rf.fit(X_train, y_train)
end_tr = time.time()

print(f"Tiempo de ajuste de hiperpar치metros Random Forest: {end_tr-start_tr:.3f} seg")


# In[24]:


# Buscar los mejores hiperpar치metros que devolvi칩 el valor m치s bajo de RECM
#--------------------------------------------------------------------------
max_score = gridSearch_rf.cv_results_['mean_test_score'].max()
index_max_score = np.where(gridSearch_rf.cv_results_['mean_test_score'] == max_score)[0][0]

best_set_params = gridSearch_rf.cv_results_['params'][index_max_score]

print(f"Mejores Hiperpar치metros del Modelo: {best_set_params} | Mejor RECM: {-max_score:.3f}")


# **ENTRENAMIENTO DEL MODELO**

# In[25]:


# Entrenamiento del Modelo Random Forest
#----------------------------

start_tr_rf = time.time()
rf_model = RandomForestRegressor(max_depth=5, n_estimators=40, random_state=42)
rf_model.fit(X_train, y_train)
end_tr_rf = time.time()

# Calculamos las Predicciones
#----------------------------
start_ts_rf = time.time()
rf_predictions = rf_model.predict(X_test)
end_ts_rf = time.time()

# Establecemos el RECM
#----------------------------------------------------
RECM_rf = mean_squared_error(y_test, rf_predictions, squared=False)
print(f'RECM de Random Forest: {RECM_rf:.3f}')
print()
print(f'Tiempo de Entrenamiento: {end_tr_rf - start_tr_rf:.3f} seg')
print(f'Tiempo de Prueba: {end_ts_rf - start_ts_rf:.3f} seg')


# ### LightGBM Regressor
# 
# El m칠todo de potenciaci칩n de gradiente LightGBM utiliza 치rboles asim칠tricos y depende de los c치lculos del 치rbol, por lo que divide la hoja del 치rbol con el mejor ajuste, de esta manera se produce una mayor precisi칩n y disminuye los errores que pueden ocurrir al utilizar el c치lculo por niveles. Al igual que Catboost, LightGBM puede procesar caracter칤sticas categ칩ricas, pero no utiliza One-Hot Encodig. Es necesario transformar las caracter칤sticas a tipo entero, ya que no acepta variables de tipo string u objeto, tambi칠n se puede optar por cambiar las caracter칤sticas categoricas a tipo Category, que es lo que vamos a realizar a continuaci칩n para entrenar nuestro modelo.

# **AJUSTE DE HIPERPAR츼METROS**

# In[26]:


# Establecemos los hiperpar치metros que vamos ajustar
#---------------------------------------------------
params_lightgbm = {'n_estimators' : [100, 500], 
                   'learning_rate': [0.1, 0.3],
                   'num_leaves'   : [31, 62],
                   'n_jobs'       : [-1]
                  }

# Llamamos al estimador que vamos a utilizar y
# Llamamos a GridSearchCV
#-------------------------------------------
lightgbm_est = LGBMRegressor()

gridSearch_lightgbm = GridSearchCV(
    estimator  = lightgbm_est,
    param_grid = params_lightgbm, 
    scoring    = 'neg_root_mean_squared_error',
    cv = tscv
)

# Entrenamos el Modelo en B칰squeda de los Hiperpar치metros que lancen el mejor RECM
#---------------------------------------------------------------------------------
start_lgbm = time.time()
gridSearch_lightgbm.fit(X_train, y_train)
end_lgbm = time.time()
print(f'Tiempo de Entrenamiento: {end_lgbm-start_lgbm:.3f} seg')


# In[27]:


# Buscamos los mejores Hipermar치metros del Modelo
#------------------------------------------------
max_score_lgbm = gridSearch_lightgbm.cv_results_['mean_test_score'].max()
index_max_score_lgbm = np.where(gridSearch_lightgbm.cv_results_['mean_test_score'] == max_score_lgbm)[0][0]

best_lgbm_set_params = gridSearch_lightgbm.cv_results_['params'][index_max_score_lgbm]

print(f"Mejores Hiperpar치metros del Modelo: {best_lgbm_set_params} | Mejor RECM: {-max_score_lgbm:.3f}")


# **ENTRENAMIENTO DEL MODELO**

# In[28]:


# Aplicamos Validaci칩n Crizada Time Series Split
#-----------------------------------------------

# Entrenamos Modelo con los Mejores Hiperpar치metros y
# Realizamos Predicciones en el Conjunto de Pruebas
#---------------------------------------------------
lightgbm_model   = LGBMRegressor(
    n_estimators = 100,
    learning_rate= 0.1,
    num_leaves   = 31,
    random_seed  = 42,
    n_jobs       = -1
)

start_ltr = time.time()
lightgbm_model.fit(X_train, y_train)
end_ltr = time.time()

start_lts = time.time()
lgbm_predictions = lightgbm_model.predict(X_test)
end_lts = time.time()

RECM_lightgbm = mean_squared_error(y_test, lgbm_predictions, squared=False)
print(f'RECM de LightGBM: {RECM_lightgbm:.3f}')
print()
print(f'Tiempo de Entrenamiento: {end_ltr-start_ltr:.3f} seg')
print(f'Tiempo de Prueba: {end_lts-start_lts:.3f} seg')


# ### XGBoost
# 
# XGBoost es un m칠todo de potenciaci칩n de gradiente que utiliza 치rboles as칤metricos al igual que LightGBM, pero se diferencia en la forma como crecen sus 치rboles, los cuales crecen por niveles no por sus hojas por lo que se obtienen 치rboles m치s complejos y grandes que hace que el modelo tome m치s tiempo en realizar predicciones y entrenarse. As칤 mismo a diferencia de LightGBM y CatBoost, XGBoost no tiene su propia implementaci칩n para caracter칤sticas categ칩ricas y solo acepta caracter칤sticas num칠ricas, por lo que requiere un preprocesamiento a trav칠s de One-Hot encoding para poder procesar datos categ칩ricos. Por esta raz칩n utilizaremos nuestro conjunto de entrenamiento, y prueba que ya fueron codificados a trav칠s de OHE para la Regresi칩n L칤neal.
# 

# **AJUSTE DE HIPERPAR츼METROS**

# In[29]:


# Establecemos los hiperpar치metros que vamos ajustar
#---------------------------------------------------
params_xgboost = {
    'booster'         : ['gbtree'],
    'max_depth'       : [6,12], 
    'n_estimators'    : [1500],
    'base_score'      : [0.5],
    'learning_rate'   : [0.01],
    'eval_metric'     : ['rmse'],
    'sampling_method' : ['uniform'],
    'subsample'       : [0.5],
    'n_jobs'          : [-1]
}

# Llamamos al estimador que vamos a utilizar y
# Llamamos a GridSearchCV
#-------------------------------------------
xgboost_est = XGBRegressor()

gridSearch_xgboost = GridSearchCV(
    estimator  = xgboost_est,
    param_grid = params_xgboost, 
    scoring    = 'neg_root_mean_squared_error',
    cv         = tscv,
    n_jobs     = -1
)

# Entrenamos el Modelo en B칰squeda de los Hiperpar치metros que lancen el mejor RECM
#---------------------------------------------------------------------------------
start_xgb = time.time()
gridSearch_xgboost.fit(X_train, y_train)
end_xgb = time.time()
print(f'Tiempo de Entrenamiento: {end_xgb-start_xgb:.3f} seg')


# In[30]:


# Buscamos los mejores Hipermar치metros del Modelo
#------------------------------------------------
max_score_xgb = gridSearch_xgboost.cv_results_['mean_test_score'].max()
index_max_score_xgb = np.where(gridSearch_xgboost.cv_results_['mean_test_score'] == max_score_xgb)[0][0]

best_xgb_set_params = gridSearch_xgboost.cv_results_['params'][index_max_score_xgb]

print(f"Mejores Hiperpar치metros del Modelo: {best_xgb_set_params} | Mejor RECM: {-max_score_xgb:.3f}")


# **ENTRENAMIENTO DEL MODELO**

# In[31]:


# Entrenamos Modelo con los Mejores Hiperpar치metros y
# Realizamos Predicciones en el Conjunto de Pruebas
#---------------------------------------------------
xgboost_model = XGBRegressor(
    base_score = 0.5,
    booster      = 'gbtree',
    eval_metric  = 'rmse',
    learning_rate= 0.01,
    n_estimators = 1500,
    max_depth    = 6,
    n_jobs       = -1,
    sampling_method = 'uniform',
    subsample    = 0.5,
    random_state = 42
)

# Entrenamiento del Modelo
#-------------------------
start_tr_xgb = time.time()
xgboost_model.fit(X_train, y_train)
end_tr_xgb = time.time()

# Predicciones
#-------------
start_ts_xgb = time.time()
xgb_predictions = xgboost_model.predict(X_test)
end_ts_xgb = time.time()


## Metricas Finales
##-----------------
RECM_xgb = mean_squared_error(y_test, xgb_predictions, squared=False)
print(f'RECM de XGBoost: {RECM_xgb:.3f}')
print()
print(f'Tiempo de Entrenamiento: {end_tr_xgb-start_tr_xgb:.3f} seg')
print(f'Tiempo de Prueba: {end_ts_xgb-start_ts_xgb:.3f} seg')


# ## Prueba
# ---
# 
# ### RECM
# 
# Una medida de uso frecuente de las diferencias entre los valores (valores de muestra o de poblaci칩n) predichos por un modelo o un estimador y los valores observados. La RECM representa la ra칤z cuadrada del segundo momento de la muestra de las diferencias entre los valores previstos y los valores observados o la media cuadr치tica de estas diferencias. Esta ser치 la m칠trica base que utilizaremos para evaluar el rendimiento de nuestros modelos.
# 
# $$RECM = \sqrt ECM = \sqrt\frac{1}{n}\sum_{i=1}^{n} (\bar{y} - y)^2 $$

# In[32]:


# Trazamos una Tabla con los Resultados del RECM obtenidos
#---------------------------------------------------------
models_table = pd.DataFrame({
    'modelo' : ['Regresi칩n_Lineal', 'Random Forest', 'LightGBM', 'XGBoost'], 
    'RECM' : [RECM_lr, RECM_rf, RECM_lightgbm, RECM_xgb]
})

models_table.sort_values(by='RECM')


# In[33]:


models_table.sort_values(by='RECM', ascending=False).plot(kind='barh', color='royalblue')
plt.title('Resultados REMC')
plt.xlabel('Puntaje de la RECM')
plt.show();


# ## || Conclusiones
# ---
# 
# Despu칠s de pasar un gran tiempo analizando este grandioso proyecto llegamos a las siguientes conclusiones:
# 
# 1. Hemos preparado los datos e importado las librer칤as necesarias para el desarrollo del proyecto.
# 2. Se corrigi칩 el tipo de datos de fechas que estaba en **Object** y se transform칩 a **Datetime**.
# 3. Se coloc칩 la columna de tiempo en 칤ndice.
# 4. Realizamos un an치lsis de nuestra tabla donde hicimos el remuestreo de nuestra tabla, adem치s estudiamos la **Desviaci칩n Est치ndar**, la **Media M칩vil** y realizamos una **Descomposici칩n STL**.
# 5. Se realizaron gr치ficas para visualizar esta investigaci칩n y se realiz칩 una prueba de **Dickey-Fuller** para averiguar si la serie temporal era estacionaria o no.
# 
# **Entrenamiento de Modelos**
# 
# 4. Se entrenaron cuatro modelos: **Linear Regresion, Random Forest, LightGBM, XGBoost**. Para cada modelo se ajustaron hiperpar치metros con la implementaci칩n de **GridSearchCV** y la validaci칩n cruzada para series de tiempo llamada **TimeSeriesSplit**, basados en los resultados de los mejores hiperpar치metros se entren칩 cada modelo y se realizaron predicciones.
# 5. El modelo con mejores resultados fue la **Regresi칩n Lineal** con una RECM de 33
